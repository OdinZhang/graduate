\section{word2vec词嵌入模型简介}

word2vec是一种比较流行的词嵌入的方法，
通过将单词转换为定长的向量以方便进行计算。
相对于One-Hot编码来说，
没有了过于稀疏的问题，
同时也可以更好地表达词与词之间的相似或类比关系。

word2vec包含有两个模型，
即跳元模型\cite{mikolovDistributedRepresentationsWords2013}
（Skip-gram）
与连续词袋模型\cite{mikolovEfficientEstimationWord2013}
（Continuous Bag of Words），
模型结构如\cref{fig:word2vec}所示。

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{images/word2vec.pdf}
    \caption{word2vec模型}\label{fig:word2vec}
\end{figure}

\subsection{跳元模型}

假设一个单词序列$W = \left\{w_1, w_2, w_3, \ldots, w_T\right\}$，
中心词为$w_t$，
上下文窗口为$2$。
则给定中心词$w_t$，
生成上下文词$w_{t-2}, w_{t-1}, w_{t+1}, w_{t+2}$的条件概率为：
\begin{equation}
    \label{eq:con}
    P(w_{t-2}, w_{t-1}, w_{t+1}, w_{t+2}|w_t)
\end{equation}

若上下文词是在给定中心词的情形下满足条件独立性，
则\cref{eq:con}可以改写为：
\begin{equation}
    P(w_{t-2}|w_t)P(w_{t-1}|w_t)P(w_{t+1}|w_t)P(w_{t+2}|w_t)
\end{equation}

令$\mathbf{v}_{w_t}\in\mathbb{R}^d$和
$\mathbf{u}_{w_t}\in\mathbb{R}^d$
表示$w_t$分别作为中心词和上下文词时的两个$d$维向量。
给定中心词$w_I$，
生成的上下文词$w_O$
的条件概率可以通过softmax函数定义：
\begin{equation}
    P\left(w_O|w_I\right) = \frac{\exp\left(\mathbf{u}_{w_O}^T\mathbf{v}_{w_I}\right)}{\sum_{w_t\in W}\exp\left(\mathbf{u}_{w_t}^T\mathbf{v}_{w_I}\right)}
\end{equation}

由于跳元模型是依据条件概率所构建的，
因此我们可以通过极大似然估计来作为模型优化方向，
即：
\begin{equation}
    \frac{1}{T}\sum^T_{t=1}\sum_{-c\leqslant j\leqslant c, j\neq 0}\log P\left(w_{t+j}|w_t\right)
\end{equation}

其中$c$为上下文窗口的大小。

\subsection{连续词袋模型}

连续词袋模型存在多个上下文词，
故需要在计算条件概率时对上下文词向量进行平均。
即令$\mathbf{v}_{w_t}\in\mathbb{R}^d$和
$\mathbf{u}_{w_t}\in\mathbb{R}^d$
表示$w_t$分别作为中心词和上下文词时的两个$d$维向量。
给定上下文词$w_{O_1}, w_{O_2}, \ldots, w_{O_{2c}}$
生成任意中心词$w_I$的条件概率可以表示为：
\begin{equation}
    P\left(w_I|w_{O_1}, \ldots, w_{O_{2c}}\right) = \frac{\exp\left(\frac{1}{2c}\mathbf{v}_I^T\left(\mathbf{u}_{O_1}+\cdots+\mathbf{u}_{O_{2c}}\right)\right)}{\sum_{w_t\in W}\exp\left(\frac{1}{2c}\mathbf{v}_{w_t}\left(\mathbf{u}_{O_1}+\cdots+\mathbf{u}_{O_{2c}}\right)\right)}
\end{equation}

由此，
连续词袋模型的损失函数可以定义为：
\begin{equation}
    -\sum_{t=1}^T\log P\left(w_t|w_{t-c}, \ldots, w_{t-1}, w_{t+1}, \ldots, w_{t+c}\right)
\end{equation}

\subsection{计算加速}

由于softmax计算的特性，
需要对词表中的每一项都进行计算，
由此可能会造成梯度的计算过于复杂，
影响模型的训练效率。
因此，
我们将引入两种近似训练方法：
负采样和分层softmax。
由于两种模型的相似性，
本文仅以跳元模型为例进行说明。

\subsubsection{噪声对比估计NCE}

NCE是Gutmann等人在2010年\cite{gutmannNoisecontrastiveEstimationNew2010}
处理非归一化模型时提出的一种参数估计原理，
以简化对归一化因子的计算。

假设集合$U=\left(\mathbf{u}_1, \ldots, \mathbf{u}_{2T}\right)$是集合
$X$和$Y$的并集，
令数据集$X$的元素个数为$T_d$，
噪声集$Y$的元素个数为$T_n$，
且给其中元素$u_t$定义标签为：
\begin{equation}
    C_t = \left\{
    \begin{aligned}
         & 1 \quad \text{if} \; \mathbf{u}_t \in X \\
         & 0 \quad \text{if} \; \mathbf{u}_t \in Y
    \end{aligned}
    \right.
\end{equation}

由于数据$\mathbf{x}$服从的概率密度函数$p_d\left(.\right)$未知，
不妨令$p_d\left(.\right)=p_m\left(.;\theta\right)$，
由此可以得到：
\begin{equation}
    p\left(\mathbf{u}|C=1;\theta\right) = p_m\left(\mathbf{u};\theta\right) \quad p\left(\mathbf{u}|C=0\right)=p_n\left(\mathbf{u}\right)
\end{equation}

先验分布为：
\begin{equation}
    \begin{aligned}
         & P\left(C=1\right)=\frac{T_d}{T_d+T_n} \\
         & P\left(C=0\right)=\frac{T_n}{T_d+T_n}
    \end{aligned}
\end{equation}

则根据全概率公式有：
\begin{equation}
    \begin{aligned}
        P\left(\mathbf{u}\right) & =P\left(\mathbf{u}|C=1;\theta\right)P\left(C=1\right)+P\left(\mathbf{u}|C=0\right)P\left(C=0\right) \\
                                 & =p_m\left(\mathbf{u};\theta\right)\frac{T_d}{T_d+T_n}+p_n\left(\mathbf{u}\right)\frac{T_n}{T_d+T_n}
    \end{aligned}
\end{equation}

再根据贝叶斯公式有：
\begin{equation}
    \begin{aligned}
        P\left(C=1|\mathbf{u};\theta\right) & =\frac{P\left(\mathbf{u}|C=1;\theta\right)P\left(C=1\right)}{P\left(\mathbf{u}\right)}                                 \\
                                            & =\frac{p_m\left(\mathbf{u};\theta\right)}{p_m\left(\mathbf{u};\theta\right)+\frac{T_n}{T_d}p_n\left(\mathbf{u}\right)}
    \end{aligned}
\end{equation}

则类别$C_t$的对数似然函数为：
\begin{equation}
    \label{eq:C_tL}
    \begin{aligned}
        L\left(\theta\right) & =\sum_{t=1}^{T_d+T_n}\left[C_t\ln P\left(C_t=1|\mathbf{u}_t;\theta\right)+\left(1-C_t\right)\ln P\left(C_t=0|\mathbf{u}_t\right)\right]        \\
                             & =\sum_{t=1}^{T_d}\ln\left[P\left(C=1|\mathbf{x}_t;\theta\right)\right]+\sum_{t=1}^{T_n}\ln\left[1-P\left(C=1|\mathbf{y}_t;\theta\right)\right]
    \end{aligned}
\end{equation}

由于\cref{eq:C_tL}与交叉熵函数的相似性，
我们可以使用逻辑回归来对模型进行学习，
从而利用相关算法达到简化计算的目的。

\subsubsection{负采样NEG}

负采样实际上是NEC的一种特殊情况，
即使$\frac{T_n}{T_d}p_n\left(\mathbf{u}\right)=1$。
把给定中心词$w_I$，
生成的上下文词$w_O$被认为是由\cref{eq:sigma}
建模的概率事件。
\begin{equation}
    \label{eq:sigma}
    \begin{aligned}
        P\left(D=1|w_I,w_O\right) & = \sigma\left(\mathbf{u}_O^T\mathbf{v}_I\right) \\
        \sigma(x)                 & =\frac{1}{1+\exp(-x)}
    \end{aligned}
\end{equation}

同理，
可以得出相应的对数损失函数。
% TODO 此处应算出相应的损失函数