\section{神经网络相关技术简介}

\subsection{M-P 神经元模型}

如\cref{fig:M-P}所示，
即为M-P神经元模型.
神经元通过接收来自其他$n$个神经元的输入信号，
通过带权重的连接进行传递，
神经元接收到的总输入值与神经元的阈值进行比较，
然后通过激活函数进行处理以产生输出\cite{zhouzhihuaJiQiXueXi}.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=.6\textwidth]{images/M-P_nn_model.pdf}
    \caption{M-P 神经元模型}\label{fig:M-P}
\end{figure}

激活函数通常使用 Sigmoid 函数和 ReLU 函数：

\begin{equation}
    \label{eq:sigmoid}
    \begin{aligned}
        Sigmoid\left(x\right) = \frac{1}{1+e^{-x}} \\
        ReLU\left(x\right) = \max\left\{0, x\right\}
    \end{aligned}
\end{equation}

其函数图像如\cref{fig:active-func}所示.

\begin{figure}
    \centering
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{images/Sigmoid.pdf}
        \subcaption{Sigmoid}\label{fig:sigmoid}
    \end{minipage}
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{images/ReLU.pdf}
        \subcaption{ReLU}\label{fig:ReLu}
    \end{minipage}
    \caption{激活函数}\label{fig:active-func}
\end{figure}

\subsection{优化算法}

通常我们会引入损失函数来衡量
神经网络的预测结果和真实结果之间的差距.
一旦拥有了损失函数，
我们便尽可能的使其结果尽可能小来进行尽可能准确的预测，
因此我们便可以使用优化算法来最小化损失.
在神经网络中常用的优化算法有：
梯度下降算法、
随机梯度下降算法、
小批量随机梯度下降算法、
动量法、
AdaGrad算法、
RMSProp算法、
Adadelta算法、
Adam算法等等.
受限于篇幅限制，
本文将着重介绍Adam算法.

\subsubsection{Adam算法}

随着深度学习技术的不断发展，
BP算法对于优化速度等方面逐渐不能满足于训练的需要，
使用随机梯度下降算法比仅仅使用梯度下降算法更加有效，
AdaGrad算法对于稀疏梯度的处理更加有效\cite{duchiAdaptiveSubgradientMethods2011}，
RMSProp算法在在线和非平稳环境下表现更好\cite{tielemanLecture5rmspropDivide2012}.
Adam算法结合了以上算法的优点，
其具体实现参照\cref{alg:adam}\cite{kingmaAdamMethodStochastic2017}.

\begin{algorithm}
    \KwData{$\alpha$：步长}
    \KwData{$\beta_1,\beta_2\in [0,1)$：非负加权参数} % chktex 9
    \KwData{$f(\theta)$：随机过程函数}
    \KwData{$\theta_0$：初始参数矢量}
    $m_0\leftarrow0$（初始化梯度动量）\;
    $v_0\leftarrow0$（初始化梯度的二阶矩）\;
    $t\leftarrow0$（初始化时间步长）\;
    \While{$\theta_t$不收敛}{
        $t\leftarrow t+1$\;
        $g_{t} \leftarrow \nabla_{\theta} f_{t}\left(\theta_{t-1}\right)$（计算当前梯度）\;
        $ m_{t} \leftarrow \beta_{1} \cdot m_{t-1}+\left(1-\beta_{1}\right) \cdot g_{t} $（更新当前梯度的动量）\;
        $ v_{t} \leftarrow \beta_{2} \cdot v_{t-1}+\left(1-\beta_{2}\right) \cdot g_{t}^{2} $（更新当前梯度的二阶矩）\;
        $ \widehat{m}_{t} \leftarrow m_{t} /\left(1-\beta_{1}^{t}\right) $（标准化梯度的动量）\;
        $ \widehat{v}_{t} \leftarrow v_{t} /\left(1-\beta_{2}^{t}\right) $（标准化梯度的二阶矩）\;
        $ \theta_{t} \leftarrow \theta_{t-1}-\alpha \cdot \widehat{m}_{t} /\left(\sqrt{\widehat{v}_{t}}+\epsilon\right) $（更新参数）\;
    }
    \KwOut{$\theta_t$}
    \caption{Adam算法}\label{alg:adam}
\end{algorithm}

\subsubsection{Adam算法优化}

虽然Adam算法成为了深度学习中最强大最有效的优化算法之一，
但是仍然有一定的问题，
比如Reddi等人\cite{reddiConvergenceAdam2019}
指出Adam算法在某些情况下存在无法收敛的情况，
因此，
我们可以使用YOGI\cite{zaheerAdaptiveMethodsNonconvex2018}
来进行热修复.

Adam算法中对于$v_t$的更新为：
\begin{equation}
    v_t \leftarrow v_{t-1}-\left(1-\beta_{2}\right)\left(v_{t-1}-g_{t}^{2}\right)
\end{equation}

YOGI算法则建议我们将其修改为：
\begin{equation}
    v_{t-1}-\left(1-\beta_{2}\right) \operatorname{sign}\left(v_{t-1}-g_{t}^{2}\right) g_{t}^{2}
\end{equation}

因此，
YOGI对Adam算法的重写如\cref{alg:yogi}所示.

\begin{algorithm}
    \KwData{$\alpha$：步长}
    \KwData{$\beta_1,\beta_2\in [0,1)$：非负加权参数} % chktex 9
    \KwData{$f(\theta)$：随机过程函数}
    \KwData{$\theta_0$：初始参数矢量}
    $m_0\leftarrow0$（初始化梯度动量）\;
    $v_0\leftarrow0$（初始化梯度的二阶矩）\;
    $t\leftarrow0$（初始化时间步长）\;
    \While{$\theta_t$不收敛}{
        $t\leftarrow t+1$\;
        $g_{t} \leftarrow \nabla_{\theta} f_{t}\left(\theta_{t-1}\right)$（计算当前梯度）\;
        $ m_{t} \leftarrow \beta_{1} \cdot m_{t-1}+\left(1-\beta_{1}\right) \cdot g_{t} $（更新当前梯度的动量）\;
        $ v_{t} \leftarrow v_{t-1}-\left(1-\beta_{2}\right) \operatorname{sign}\left(v_{t-1}-g_{t}^{2}\right) g_{t}^{2} $（更新当前梯度的二阶矩）\;
        $ \widehat{m}_{t} \leftarrow m_{t} /\left(1-\beta_{1}^{t}\right) $（标准化梯度的动量）\;
        $ \widehat{v}_{t} \leftarrow v_{t} /\left(1-\beta_{2}^{t}\right) $（标准化梯度的二阶矩）\;
        $ \theta_{t} \leftarrow \theta_{t-1}-\alpha \cdot \widehat{m}_{t} /\left(\sqrt{\widehat{v}_{t}}+\epsilon\right) $（更新参数）\;
    }
    \KwOut{$\theta_t$}
    \caption{YOGI算法}\label{alg:yogi}
\end{algorithm} % chktex 17
