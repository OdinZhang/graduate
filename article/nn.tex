\section{神经网络相关技术简介}

\subsection{M-P 神经元模型}

如\cref{fig:M-P}所示，
即为M-P神经元模型.
神经元通过接收来自其他$n$个神经元的输入信号，
通过带权重的连接进行传递，
神经元接收到的总输入值与神经元的阈值进行比较，
然后通过激活函数进行处理以产生输出\cite{zhouzhihuaJiQiXueXi}.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=.6\textwidth]{images/M-P_nn_model.pdf}
    \caption{M-P 神经元模型}\label{fig:M-P}
\end{figure}

激活函数通常使用 Sigmoid 函数和 ReLU 函数：

\begin{equation}
    \label{eq:sigmoid}
    \begin{aligned}
        Sigmoid\left(x\right) = \frac{1}{1+e^{-x}} \\
        ReLU\left(x\right) = \max\left\{0, x\right\}
    \end{aligned}
\end{equation}

其函数图像如\cref{fig:active-func}所示.

\begin{figure}
    \centering
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{images/Sigmoid.pdf}
        \subcaption{Sigmoid}
        \label{fig:sigmoid}
    \end{minipage}
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{images/ReLU.pdf}
        \subcaption{ReLU}
        \label{fig:ReLu}
    \end{minipage}
    \caption{多图并排示例}
    \label{fig:active-func}
\end{figure}

\subsection{优化算法}

通常我们会引入损失函数来衡量
神经网络的预测结果和真实结果之间的差距。
一旦拥有了损失函数，
我们便尽可能的使其结果尽可能小来进行尽可能准确的预测，
因此我们便可以使用优化算法来最小化损失。
在神经网络中常用的优化算法有：
梯度下降算法、
随机梯度下降算法、
小批量随机梯度下降算法、
动量法、
AdaGrad算法、
RMSProp算法、
Adadelta算法、
Adam算法等等。
受限于篇幅限制，
本文将着重介绍较为常用的梯度下降算法的实现---
反向传播算法与Adam算法。

\subsubsection{反向传播算法}

由于多层网络的学习能力通常要强于单层感知机，
因此，
需要一种更加强大的算法.
最常用的就是反向传播算法（BP算法），
其不仅可以用于多层前馈神经网络，
同样可以用于诸如递归神经网络等其余类型的网络.
通常情况下“BP 神经网络”指使用 BP 算法训练的多层前馈神经网络.

假定一个前馈神经网络具有$d$个输入神经元，
$l$个输出神经元，
$q$个隐层神经元，
其中输出层第$j$个神经元的阈值为$\theta_j$，
隐层第$h$个神经元的阈值为$\gamma_h$.
记输入层第$i$个神经元与隐层第$h$个神经元的连接权为$v_{ih}$，
隐层层第$h$个神经元与输出层第$j$个神经元的连接权为$w_{ih}$.
记隐层第$h$个神经元接收到的输入为$\alpha_h=\sum_{i=1}^dv_{ih}x_i$，
输出层第$j$个神经元的输出为$\beta_j=\sum_{h=1}^{q}{w_{hj}b_h}$，
$b_h$为隐层第$h$个神经元的输出.
假设隐层和输出层都使用 Sigmoid 函数.

对训练例$\left(\mathbf{x}_k, \mathbf{y}_k\right)$，
假定输出为$\hat{\mathbf{y}}_k=\left(\hat{y}_1^k, \hat{y}_2^k, \ldots, \hat{y}_l^k\right)$，
即:

\begin{equation}
    \label{eq:yjk}
    \hat{y}_j^k = f\left(\beta_j-\theta_j\right)
\end{equation}

则均方误差为：

\begin{equation}
    \label{eq:mse}
    E_k=\frac{1}{2}\sum_{j=1}^l{\left(\hat{y}_l^k-y_j^k\right)^2}
\end{equation}

根据\cref{eq:mse}，
以目标的负梯度方向对参数进行调整.
给定学习率$\eta$，
有：

\begin{equation}
    \Delta w_{hj}=\eta\frac{\partial{E_k}}{\partial{w_{hj}}}
\end{equation}

\begin{equation}
    \label{eq:partial}
    \frac{\partial{E_k}}{\partial{w_{hj}}}=\frac{\partial{E_k}}{\partial{\hat{y}_j^k}}\cdot\frac{\partial{\hat{y}_j^k}}{\partial{\beta_j}}\cdot\frac{\partial{\beta_j}}{\partial{w_{hj}}}
\end{equation}

由$\beta$的定义可以得出：

\begin{equation}
    \label{eq:b}
    \frac{\partial \beta_j}{\partial w_{hj}} = b_h
\end{equation}

对于\cref{eq:sigmoid}中 Sigmoid 来说，
有：

\begin{equation}
    f^{\prime}\left(x\right) = f\left(x\right)\left(1-f\left(x\right)\right)
\end{equation}

于是根据\cref{eq:yjk}和\cref{eq:mse}，
有：

\begin{equation}
    \label{eq:g}
    \begin{aligned}
        g _ { j } & = - \frac { \partial E _ { k } } { \partial \hat { y } _ { j } ^ { k } } \cdot \frac { \partial \hat { y } _ { j } ^ { k } } { \partial \beta _ { j } } \\
                  & = - ( \hat { y } _ { j } ^ { k } - y _ { j } ^ { k } ) f ^ { \prime } ( \beta _ { j } - \theta _ { j } )                                                \\
                  & = \hat { y } _ { j } ^ { k } ( 1 - \hat { y } _ { j } ^ { k } ) ( y _ { j } ^ { k } - \hat { y } _ { j } ^ { k } )
    \end{aligned}
\end{equation}

把\cref{eq:g}和\cref{eq:b}代入\cref{eq:partial}，
即可得出 BP 算法中关于$w_{ij}$的更新公式：

\begin{equation}
    \label{eq:whj}
    \Delta w_{hj} = \eta g_jb_h
\end{equation}

同理可得：

\begin{align}
    \Delta\theta_j                   & = -\eta g_j   \\
    \label{eq:vih}\Delta v_{ih}      & = \eta e_hx_i \\
    \label{eq:gammah}\Delta \gamma_h & = -\eta e_h
\end{align}

\cref{eq:vih}和\cref{eq:gammah}中：

\begin{equation}
    \label{eq:e}
    \begin{aligned}
        e _ { h } & = - \frac { \partial E _ { k } } { \partial b _ { h } } \cdot \frac { \partial b _ { h } } { \partial \alpha _ { h } }                                                                                \\
                  & = - \sum _ { j = 1 } ^ { l } \frac { \partial E _ { k } } { \partial \beta _ { j } } \cdot \frac { \partial \beta _ { j } } { \partial b _ { h } } f ^ { \prime } ( \alpha _ { h } - \gamma _ { h } ) \\
                  & = \sum _ { j = 1 } ^ { l } w _ { h j } g _ { j } f ^ { \prime } ( \alpha _ { h } - \gamma _ { h } )                                                                                                   \\
                  & = b _ { h } ( 1 - b _ { h } ) \sum _ { j = 1 } ^ { l } w _ { h j } g _ { j }
    \end{aligned}
\end{equation}

可以得到计算的算法如\cref{alg:nn}所示\cite{zhouzhihuaJiQiXueXi}.

\begin{algorithm}
    \KwIn{训练集$D=\left\{(x_k, y_k)\right\}^m_{k=1}$; 学习率$\eta$.}
    在$(0, 1)$范围内随机初始化网络中所有链接权和阈值\;
    \Repeat{达到停止条件}{
        \ForAll{$(\mathbf{x}_k, \mathbf{y}_k) \in D$}{
            根据当前参数和\cref{eq:yjk}计算当前样本的输出$\hat{\mathbf{y}}_k$\;
            根据\cref{eq:g}计算输出层神经元的梯度项$g_j$\;
            根据\cref{eq:e}计算隐层神经元的梯度项$e_h$\;
            根据\cref{eq:whj}~--\cref{eq:gammah}更新连接权$w_{hj}$，$v_{ih}$与阈值$\theta_j$，$\gamma_h$
        }
    }
    \KwOut{连接权与阈值确定的多层前馈神经网络}
    \caption{反向传播算法}
    \label{alg:nn}
\end{algorithm}

\subsubsection{Adam算法}

随着深度学习技术的不断发展，
BP算法对于优化速度等方面逐渐不能满足于训练的需要，
使用随机梯度下降算法比仅仅使用梯度下降算法更加有效，
AdaGrad算法对于稀疏梯度的处理更加有效\cite{duchiAdaptiveSubgradientMethods2011}，
RMSProp算法在在线和非平稳环境下表现更好\cite{tielemanLecture5rmspropDivide2012}。
Adam算法结合了以上算法的优点，
其具体实现参照\cref{alg:adam}\cite{kingmaAdamMethodStochastic2017}。

\begin{algorithm}
    \KwData{$\alpha$：步长}
    \KwData{$\beta_1,\beta_2\in [0,1)$：非负加权参数}
    \KwData{$f(\theta)$：随机过程函数}
    \KwData{$\theta_0$：初始参数矢量}
    $m_0\leftarrow0$（初始化梯度动量）\;
    $v_0\leftarrow0$（初始化梯度的二阶矩）\;
    $t\leftarrow0$（初始化时间步长）\;
    \While{$\theta_t$不收敛}{
        $t\leftarrow t+1$\;
        $g_{t} \leftarrow \nabla_{\theta} f_{t}\left(\theta_{t-1}\right)$（计算当前梯度）\;
        $ m_{t} \leftarrow \beta_{1} \cdot m_{t-1}+\left(1-\beta_{1}\right) \cdot g_{t} $（更新当前梯度的动量）\;
        $ v_{t} \leftarrow \beta_{2} \cdot v_{t-1}+\left(1-\beta_{2}\right) \cdot g_{t}^{2} $（更新当前梯度的二阶矩）\;
        $ \widehat{m}_{t} \leftarrow m_{t} /\left(1-\beta_{1}^{t}\right) $（标准化梯度的动量）\;
        $ \widehat{v}_{t} \leftarrow v_{t} /\left(1-\beta_{2}^{t}\right) $（标准化梯度的二阶矩）\;
        $ \theta_{t} \leftarrow \theta_{t-1}-\alpha \cdot \widehat{m}_{t} /\left(\sqrt{\widehat{v}_{t}}+\epsilon\right) $（更新参数）\;
    }
    \KwOut{$\theta_t$}
    \caption{Adam算法}
    \label{alg:adam}
\end{algorithm}

\subsection{分类问题的处理}

对于二分类问题，
可以对输出神经元设定阈值$k$与激活函数$f$，
当输出值$\hat{y}>k\;(0\leqslant\hat{y}\leqslant1)$时，
输出$1$，
否则输出$0$.
